{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the reddit API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://www.reddit.com/'\n",
    "data = {'grant_type': 'password', 'username': 'liloho', 'password': 'Winter05'}\n",
    "auth = requests.auth.HTTPBasicAuth('vNW8cTO2bw2jJA', '2abo8mFQWCHczgBIMha34yqcjdE0Jw')\n",
    "r = requests.post(base_url + 'api/v1/access_token',\n",
    "                  data=data,\n",
    "                  headers={'user-agent': 'APP-NAME by REDDIT-USERNAME'},\n",
    "                  auth=auth)\n",
    "d = r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'access_token': '524798392816-7hLQGMAXBv52L7t-RtWjzJ0dkm4zLw',\n",
       " 'token_type': 'bearer',\n",
       " 'expires_in': 3600,\n",
       " 'scope': '*'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "liloho 0\n"
     ]
    }
   ],
   "source": [
    "token = 'bearer ' + d['access_token']\n",
    "\n",
    "base_url = 'https://oauth.reddit.com'\n",
    "\n",
    "headers = {'Authorization': token, 'User-Agent': 'APP-NAME by REDDIT-USERNAME'}\n",
    "response = requests.get(base_url + '/api/v1/me', headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(response.json()['name'], response.json()['comment_karma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "payload = {'q': 'bread', 'limit': 20, 'sort': 'relevance'}\n",
    "response = requests.get(base_url + '/subreddits/search', headers=headers, params=payload)\n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bread\n",
      "BreadStapledToTrees\n",
      "GarlicBreadMemes\n",
      "LifeProTips\n",
      "grilledcheese\n",
      "breaddit\n",
      "ketorecipes\n",
      "eatsandwiches\n",
      "mildlyinteresting\n",
      "Breadit\n",
      "mildlyinfuriating\n",
      "shittyfoodporn\n",
      "Baking\n",
      "memes\n",
      "ArtisanBread\n",
      "BreadMachines\n",
      "COMPLETEANARCHY\n",
      "BreadTube\n",
      "garlicbread\n",
      "food\n",
      "Sourdough\n",
      "Tinder\n",
      "bread_irl\n",
      "FoodPorn\n",
      "Showerthoughts\n",
      "teenagers\n",
      "6thForm\n",
      "pics\n",
      "funny\n",
      "AskReddit\n",
      "todayilearned\n",
      "aww\n",
      "keto\n",
      "Cooking\n",
      "castiron\n",
      "52weeksofcooking\n",
      "vegan\n",
      "1200isplenty\n",
      "ShittyVeganFoodPorn\n",
      "dankmemes\n",
      "AskCulinary\n",
      "britishproblems\n",
      "glutenfree\n",
      "unpopularopinion\n",
      "tonightsdinner\n",
      "PewdiepieSubmissions\n",
      "recipes\n",
      "52weeksofbaking\n",
      "aaaaaaacccccccce\n",
      "Catloaf\n",
      "trees\n",
      "forbiddensnacks\n",
      "oddlysatisfying\n",
      "Old_Recipes\n",
      "Keto_Food\n",
      "tf2\n",
      "Jokes\n",
      "covidcookery\n",
      "subway\n",
      "WeWantPlates\n",
      "veganrecipes\n",
      "jimmyjohns\n",
      "VeganFoodPorn\n",
      "blursedimages\n",
      "Sandwiches\n",
      "NoStupidQuestions\n",
      "cats\n",
      "videos\n",
      "HealthyFood\n",
      "TIHI\n",
      "Pizza\n",
      "CasualUK\n",
      "shittysuperpowers\n",
      "Minecraft\n",
      "WTF\n",
      "Celiac\n",
      "dadjokes\n",
      "drunkencookery\n",
      "MealPrepSunday\n",
      "BakingNoobs\n",
      "GifRecipes\n",
      "EatCheapAndHealthy\n",
      "Wellthatsucks\n",
      "KitchenConfidential\n",
      "tumblr\n",
      "shittyaskscience\n",
      "cursedcomments\n",
      "ketoaustralia\n",
      "explainlikeimfive\n",
      "bingingwithbabish\n",
      "teenagersnew\n",
      "Frugal\n",
      "Homebrewing\n",
      "ireland\n",
      "traderjoes\n",
      "FreeKarma4U\n",
      "RedditInTheKitchen\n",
      "AdviceAnimals\n",
      "firstworldproblems\n",
      "HistoryMemes\n"
     ]
    }
   ],
   "source": [
    "values = response.json()\n",
    "#print(values)\n",
    "\n",
    "for i in range(len(values['data']['children'])):\n",
    "    print(values['data']['children'][i]['data']['display_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using praw\n",
    "\n",
    "Official documentation: https://praw.readthedocs.io/en/latest/code_overview/models/submission.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.geeksforgeeks.org/python-praw-python-reddit-api-wrapper/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import praw\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "reddit = praw.Reddit(client_id ='vNW8cTO2bw2jJA', \n",
    "                     client_secret ='2abo8mFQWCHczgBIMha34yqcjdE0Jw', \n",
    "                     user_agent ='my user agent', \n",
    "                     username ='liloho', \n",
    "                     password ='Winter05') \n",
    "  \n",
    "# to verify whether the instance is authorised instance or not \n",
    "print(reddit.read_only) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access a Subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sourdough\n",
      "Sourdough\n",
      "Want to learn how to make and bake sourdough? Love the aroma, taste, and texture of homemade bread? If yes, this is your subreddit!  Ask questions, start discussions, share recipes, photos, baking tips, and much more.\n",
      "\n",
      "***\n",
      "\n",
      "Get your own custom bread flair by clicking \"edit\" just above and selecting your flair.\n",
      "\n",
      "***\n",
      "\n",
      "**Rules:**\n",
      "\n",
      "* Be polite & respectful\n",
      "* No submitting irrelevant content (including memes)\n",
      "* No spamming\n",
      "* [Reddiquette](http://www.reddit.com/help/reddiquette)\n",
      "\n",
      "***\n",
      "\n",
      "**Resources:**\n",
      "\n",
      "* [Beginner's Guide to Sourdough](/r/Sourdough/wiki/starter_culture_resources)\n",
      "* [General Information on Sourdough](/r/Sourdough/wiki/general_information)\n",
      "* [Basic Troubleshooting](https://i.redd.it/w3ami1gnyqr41.jpg)\n",
      "* [Sourdough Recipe w/ Wakthrough](/r/Sourdough/wiki/standard-sd-recipe)\n",
      "\n",
      "***\n",
      "\n",
      "**Baking Related Subreddits:**\n",
      "\n",
      "* [/r/ArtisanBread](http://www.reddit.com/r/ArtisanBread/)\n",
      "* [/r/Breadit](http://www.reddit.com/r/Breadit/)\n",
      "* [/r/Pizza](http://www.reddit.com/r/Pizza/)\n",
      "* [/r/Baking](http://www.reddit.com/r/Baking/)\n",
      "\n",
      "***\n",
      "\n",
      "**Other Related Subreddits:**\n",
      "\n",
      "* [/r/AskCulinary ](http://www.reddit.com/r/AskCulinary/)\n",
      "* [/r/Charcuterie](http://www.reddit.com/r/charcuterie/)\n",
      "* [/r/Cheese](http://www.reddit.com/r/cheese/)\n",
      "* [/r/Condiments](http://www.reddit.com/r/condiments/)\n",
      "* [/r/EatSandwiches](http://www.reddit.com/r/eatsandwiches)\n",
      "* [/r/Fermentation](http://www.reddit.com/r/fermentation/)\n"
     ]
    }
   ],
   "source": [
    "subreddit = reddit.subreddit('sourdough') \n",
    "  \n",
    "# display the subreddit name \n",
    "print(subreddit.display_name) \n",
    "  \n",
    "# display the subreddit title  \n",
    "print(subreddit.title)        \n",
    "  \n",
    "# display the subreddit description  \n",
    "print(subreddit.description) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:  Here’s another video of me shaping sourdough. I added some music this time because baking is rock ’n roll.\n",
      "Score:  4435\n",
      "ID:  glzuwy\n",
      "URL:  https://v.redd.it/t8jaoor0giz41\n",
      "Created:  1589801997.0\n",
      "Number of comments:  214\n"
     ]
    }
   ],
   "source": [
    "# to find the top most submission in the subreddit \"sourdough\" \n",
    "subreddit = reddit.subreddit('sourdough') \n",
    "  \n",
    "for submission in subreddit.top(limit = 1): \n",
    "    # displays the submission title \n",
    "    print(\"Title: \", submission.title)   \n",
    "  \n",
    "    # displays the net upvotes of the submission \n",
    "    print(\"Score: \", submission.score)   \n",
    "  \n",
    "    # displays the submission's ID \n",
    "    print(\"ID: \", submission.id)    \n",
    "  \n",
    "    # displays the url of the submission \n",
    "    print(\"URL: \", submission.url) \n",
    "    \n",
    "    # displays when the submission was created in unix time\n",
    "    print(\"Created: \", submission.created_utc)  \n",
    "    \n",
    "    # displays number of comments to the submission\n",
    "    print(\"Number of comments: \", submission.num_comments) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['An olive rosemary loaf! First sourdough bake of the year & first time adding inclusions was a success. The aromatics & flavor of this loaf is insane, excited to experiment more w/ inclusions this year.',\n",
       " 'Delighted with the crumb on this one!',\n",
       " 'My first sourdough bread.',\n",
       " 'Sourdough newbie here',\n",
       " 'Below is a slice from the end, top is a slice from the middle of the same bread. What went wrong?']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to find the top most submission in the subreddit \"sourdough\" \n",
    "subreddit = reddit.subreddit('sourdough') \n",
    "\n",
    "df_title = []\n",
    "\n",
    "for submission in subreddit.new(limit = 5): \n",
    "    title = submission.title\n",
    "    df_title.append(title)\n",
    "\n",
    "df_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get latest, top or hottest submission in reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert unix time into date --> not working properly, need to fix unix conversion\n",
    "#def get_yyyy_mm_dd_from_utc(dt):\n",
    "#    date = datetime.utcfromtimestamp(dt)\n",
    "#    return str(date.year) + \"-\" + str(date.month) + \"-\" + str(date.day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find the top most submission in the subreddit \"sourdough\" \n",
    "subreddit = reddit.subreddit('sourdough') \n",
    "\n",
    "topics_dict = { \"title\":[], \n",
    "               \"score\":[],\n",
    "              \"created_UTC\": [],\n",
    "               \"num_comments\": [],\n",
    "               \"comments_text\": []}\n",
    "\n",
    "for submission in subreddit.top(limit = 10): \n",
    "    topics_dict[\"title\"].append(submission.title)\n",
    "    topics_dict[\"score\"].append(submission.score)\n",
    "    topics_dict[\"created_UTC\"].append(submission.created)\n",
    "    topics_dict[\"num_comments\"].append(submission.num_comments)\n",
    "    \n",
    "    #https://praw.readthedocs.io/en/latest/tutorials/comments.html\n",
    "    submission.comments.replace_more(limit=None)\n",
    "    comment_body = \"\"\n",
    "    for comment in submission.comments.list():\n",
    "        comment_body =  comment_body + comment.body + \"\\n\"\n",
    "    topics_dict[\"comments_text\"].append(comment_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>created_UTC</th>\n",
       "      <th>created_date</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>comments_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Here’s another video of me shaping sourdough. ...</td>\n",
       "      <td>4427</td>\n",
       "      <td>1.589831e+09</td>\n",
       "      <td>2020-5-18</td>\n",
       "      <td>214</td>\n",
       "      <td>This is hypnotizing, and I’m so impressed with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My dad decided to make a Coronavirus themed so...</td>\n",
       "      <td>3359</td>\n",
       "      <td>1.589723e+09</td>\n",
       "      <td>2020-5-17</td>\n",
       "      <td>44</td>\n",
       "      <td>it's got such a 1990's aesthetic to it, i love...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>by request: a timelapse of me scoring my sunfl...</td>\n",
       "      <td>3294</td>\n",
       "      <td>1.584190e+09</td>\n",
       "      <td>2020-3-14</td>\n",
       "      <td>104</td>\n",
       "      <td>Ngl I thought the cutting was over about 10 di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How else was I suppose to announce that we are...</td>\n",
       "      <td>2918</td>\n",
       "      <td>1.591844e+09</td>\n",
       "      <td>2020-6-11</td>\n",
       "      <td>86</td>\n",
       "      <td>Congrats on the sex!\\nCongratulations on loaf ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Today is a good day.</td>\n",
       "      <td>2843</td>\n",
       "      <td>1.588969e+09</td>\n",
       "      <td>2020-5-8</td>\n",
       "      <td>121</td>\n",
       "      <td>It's so pretty I want to cry\\nAmazing ! What w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I finally achieved what I thought was impossib...</td>\n",
       "      <td>2836</td>\n",
       "      <td>1.600390e+09</td>\n",
       "      <td>2020-9-18</td>\n",
       "      <td>165</td>\n",
       "      <td>Do you use gluten free flour in the banneton? ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>That incredible moment when you take off the t...</td>\n",
       "      <td>2647</td>\n",
       "      <td>1.590562e+09</td>\n",
       "      <td>2020-5-27</td>\n",
       "      <td>119</td>\n",
       "      <td>That bread has never even heard of gluten\\nOn ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>My pizza dough this morning</td>\n",
       "      <td>2530</td>\n",
       "      <td>1.593475e+09</td>\n",
       "      <td>2020-6-29</td>\n",
       "      <td>78</td>\n",
       "      <td>holy gluten batman.\\nWould you mind sharing yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Hey guys here’s a video of me dividing and pre...</td>\n",
       "      <td>2525</td>\n",
       "      <td>1.590619e+09</td>\n",
       "      <td>2020-5-27</td>\n",
       "      <td>109</td>\n",
       "      <td>I find every one of your videos so hypnoticall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>✨ a little project I did tonight for my kitche...</td>\n",
       "      <td>2341</td>\n",
       "      <td>1.593784e+09</td>\n",
       "      <td>2020-7-3</td>\n",
       "      <td>113</td>\n",
       "      <td>[deleted]\\nYou forgot 10% Luck, 20% Skill\\nThi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  score   created_UTC  \\\n",
       "0  Here’s another video of me shaping sourdough. ...   4427  1.589831e+09   \n",
       "1  My dad decided to make a Coronavirus themed so...   3359  1.589723e+09   \n",
       "2  by request: a timelapse of me scoring my sunfl...   3294  1.584190e+09   \n",
       "3  How else was I suppose to announce that we are...   2918  1.591844e+09   \n",
       "4                               Today is a good day.   2843  1.588969e+09   \n",
       "5  I finally achieved what I thought was impossib...   2836  1.600390e+09   \n",
       "6  That incredible moment when you take off the t...   2647  1.590562e+09   \n",
       "7                        My pizza dough this morning   2530  1.593475e+09   \n",
       "8  Hey guys here’s a video of me dividing and pre...   2525  1.590619e+09   \n",
       "9  ✨ a little project I did tonight for my kitche...   2341  1.593784e+09   \n",
       "\n",
       "  created_date  num_comments  \\\n",
       "0    2020-5-18           214   \n",
       "1    2020-5-17            44   \n",
       "2    2020-3-14           104   \n",
       "3    2020-6-11            86   \n",
       "4     2020-5-8           121   \n",
       "5    2020-9-18           165   \n",
       "6    2020-5-27           119   \n",
       "7    2020-6-29            78   \n",
       "8    2020-5-27           109   \n",
       "9     2020-7-3           113   \n",
       "\n",
       "                                       comments_text  \n",
       "0  This is hypnotizing, and I’m so impressed with...  \n",
       "1  it's got such a 1990's aesthetic to it, i love...  \n",
       "2  Ngl I thought the cutting was over about 10 di...  \n",
       "3  Congrats on the sex!\\nCongratulations on loaf ...  \n",
       "4  It's so pretty I want to cry\\nAmazing ! What w...  \n",
       "5  Do you use gluten free flour in the banneton? ...  \n",
       "6  That bread has never even heard of gluten\\nOn ...  \n",
       "7  holy gluten batman.\\nWould you mind sharing yo...  \n",
       "8  I find every one of your videos so hypnoticall...  \n",
       "9  [deleted]\\nYou forgot 10% Luck, 20% Skill\\nThi...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert dictionary to dataframe\n",
    "topics_data = pd.DataFrame(topics_dict)\n",
    "topics_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using pushshift.io\n",
    "\n",
    "Official documentation: https://pushshift.io/api-parameters/\n",
    "\n",
    "Useful resources:\n",
    "\n",
    "- https://stackoverflow.com/questions/53988619/praw-6-get-all-submission-of-a-subreddit\n",
    "- https://medium.com/@pasdan/how-to-scrap-reddit-using-pushshift-io-via-python-a3ebcc9b83f4\n",
    "- https://rareloot.medium.com/using-pushshifts-api-to-extract-reddit-submissions-fb517b286563"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import requests\n",
    "import itertools\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use logic to request more than 500 posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_request(uri, max_retries = 5):\n",
    "    def fire_away(uri):\n",
    "        response = requests.get(uri)\n",
    "        assert response.status_code == 200\n",
    "        return json.loads(response.content)    \n",
    "    current_tries = 1\n",
    "    while current_tries < max_retries:\n",
    "        try:\n",
    "            time.sleep(1)\n",
    "            response = fire_away(uri)\n",
    "            return response\n",
    "        except:\n",
    "            time.sleep(1)\n",
    "            current_tries += 1    \n",
    "    return fire_away(uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_posts_for(subreddit, start_at, end_at):\n",
    "    \n",
    "    def map_posts(posts):\n",
    "        return list(map(lambda post: {\n",
    "            'id': post['id'],\n",
    "            'created_utc': post['created_utc'],\n",
    "            'prefix': 't4_'\n",
    "        }, posts))\n",
    "    \n",
    "    SIZE = 500\n",
    "    URI_TEMPLATE = r'https://api.pushshift.io/reddit/search/submission?subreddit={}&after={}&before={}&size={}'\n",
    "    \n",
    "    post_collections = map_posts( \\\n",
    "        make_request(URI_TEMPLATE.format(subreddit, start_at, end_at, SIZE))['data'])    \n",
    "    \n",
    "    n = len(post_collections)\n",
    "    while n == SIZE:\n",
    "        last = post_collections[-1]\n",
    "        new_start_at = last['created_utc'] - (10)\n",
    "        \n",
    "        more_posts = map_posts( \\\n",
    "            make_request(URI_TEMPLATE.format(subreddit, new_start_at, end_at, SIZE))['data'])\n",
    "        \n",
    "        n = len(more_posts)\n",
    "        post_collections.extend(more_posts)\n",
    "        \n",
    "    return post_collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building time period search intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_me_intervals(start_at, end_at, days_per_interval): \n",
    "           \n",
    "    period = (86400 * days_per_interval)    ## 1 day = 86400\n",
    "    end = start_at + period\n",
    "    yield (int(start_at), int(end))    \n",
    "    padding = 1 \n",
    "    while end <= end_at:\n",
    "        start_at = end + padding\n",
    "        end = (start_at - padding) + period\n",
    "        yield int(start_at), int(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length: 732\n",
      "[(1577836800, 1578441600), (1578441601, 1579046400), (1579046401, 1579651200), (1579651201, 1580256000), (1580256001, 1580860800), (1580860801, 1581465600), (1581465601, 1582070400), (1582070401, 1582675200), (1582675201, 1583280000), (1583280001, 1583884800), (1583884801, 1584489600), (1584489601, 1585094400), (1585094401, 1585699200), (1585699201, 1586304000), (1586304001, 1586908800), (1586908801, 1587513600), (1587513601, 1588118400), (1588118401, 1588723200), (1588723201, 1589328000), (1589328001, 1589932800), (1589932801, 1590537600), (1590537601, 1591142400), (1591142401, 1591747200), (1591747201, 1592352000), (1592352001, 1592956800), (1592956801, 1593561600), (1593561601, 1594166400), (1594166401, 1594771200), (1594771201, 1595376000), (1595376001, 1595980800), (1595980801, 1596585600), (1596585601, 1597190400), (1597190401, 1597795200), (1597795201, 1598400000), (1598400001, 1599004800), (1599004801, 1599609600), (1599609601, 1600214400), (1600214401, 1600819200), (1600819201, 1601424000), (1601424001, 1602028800), (1602028801, 1602633600), (1602633601, 1603238400), (1603238401, 1603843200), (1603843201, 1604448000), (1604448001, 1605052800), (1605052801, 1605657600), (1605657601, 1606262400), (1606262401, 1606867200), (1606867201, 1607472000), (1607472001, 1608076800), (1608076801, 1608681600), (1608681601, 1609286400), (1609286401, 1609891200)]\n"
     ]
    }
   ],
   "source": [
    "## test function\n",
    "start_at = math.floor((datetime(2020, 1, 1, 0, 0, 0)).timestamp())\n",
    "end_at = math.ceil(c\n",
    "\n",
    "print(\"length:\", len(list(give_me_intervals(start_at, end_at, 0.5))))\n",
    "print(list(give_me_intervals(start_at,end_at, 7)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3146\n",
      "3146\n"
     ]
    }
   ],
   "source": [
    "# Define parameters\n",
    "#To be safe, I changed the day interval to 1/2 day, in case any day had more than 100 posts\n",
    "subreddit = 'Sourdough'\n",
    "start_at = math.floor((datetime(2020, 1, 1, 0, 0, 0)).timestamp())\n",
    "end_at = math.ceil(datetime(2020, 12, 31, 23, 59, 59).timestamp()) \n",
    "days_per_interval = 0.5\n",
    "\n",
    "posts = []\n",
    "for interval in give_me_intervals(start_at, end_at, days_per_interval):\n",
    "    pulled_posts = pull_posts_for(subreddit, interval[0], interval[1])\n",
    "    posts.extend(pulled_posts)\n",
    "    time.sleep(.500)\n",
    "\n",
    "# check results\n",
    "print(len(posts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# write the result to a file in case I want to use it later to avoid having to rerun the code above\n",
    "import pickle\n",
    "\n",
    "my_object = posts\n",
    "pickle_out = open(\"posts_list.pickle\",\"wb\")\n",
    "pickle.dump(my_object, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'eibhvl', 'created_utc': 1577839131, 'prefix': 't4_'},\n",
       " {'id': 'eibvur', 'created_utc': 1577841129, 'prefix': 't4_'},\n",
       " {'id': 'eiby7m', 'created_utc': 1577841483, 'prefix': 't4_'},\n",
       " {'id': 'eictkk', 'created_utc': 1577846281, 'prefix': 't4_'},\n",
       " {'id': 'eidmqm', 'created_utc': 1577851082, 'prefix': 't4_'},\n",
       " {'id': 'eidtic', 'created_utc': 1577852213, 'prefix': 't4_'},\n",
       " {'id': 'eidxpd', 'created_utc': 1577852956, 'prefix': 't4_'},\n",
       " {'id': 'eidyxu', 'created_utc': 1577853173, 'prefix': 't4_'},\n",
       " {'id': 'eifrvq', 'created_utc': 1577864698, 'prefix': 't4_'},\n",
       " {'id': 'eigw2g', 'created_utc': 1577873535, 'prefix': 't4_'}]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Unpickling: read result back in using pickle\n",
    "#pickle_in = open(\"posts_list.pickle\",\"rb\")\n",
    "#test_object = pickle.load(pickle_in)\n",
    "#test_object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data on posts using the pulled submission id and praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3146\n"
     ]
    }
   ],
   "source": [
    "# Generate list including Submissions and their id to then get the rest of the data from\n",
    "posts_from_reddit = []\n",
    "\n",
    "for submission_id in np.unique([ post['id'] for post in posts ]):\n",
    "    submission = reddit.submission(id=submission_id) \n",
    "    \n",
    "    posts_from_reddit.append(submission)  \n",
    "\n",
    "print(len(posts_from_reddit))\n",
    "print(posts_from_reddit[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull other data from reddit including title, score, date, number of comments, etc\n",
    "posts_dict = { \"title\":[], \n",
    "               \"score\":[],\n",
    "              \"created\": [],\n",
    "               \"created_UTC\": [],\n",
    "               \"num_comments\": [],\n",
    "               \"comments_text\": []\n",
    "              }\n",
    "\n",
    "for submission in posts_from_reddit:\n",
    "    topics_dict[\"title\"].append(submission.title)\n",
    "    topics_dict[\"score\"].append(submission.score)\n",
    "    topics_dict[\"created_UTC\"].append(submission.created)\n",
    "    topics_dict[\"num_comments\"].append(submission.num_comments)\n",
    "\n",
    "    \n",
    "#convert dictionary to dataframe\n",
    "posts_data = pd.DataFrame(posts_dict)\n",
    "posts_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert unix time into date --> not working properly, need to fix unix conversion\n",
    "def get_yyyy_mm_dd_from_utc(dt):\n",
    "    date = datetime.utcfromtimestamp(dt)\n",
    "    return str(date.year) + \"-\" + str(date.month) + \"-\" + str(date.day)\n",
    "\n",
    "created = []\n",
    "\n",
    "for submission in posts_from_reddit:\n",
    "    created.append(get_yyyy_mm_dd_from_utc(submission.created))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(created).to_csv(\"date.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list(np.unique([ post['id'] for post in posts ]))).to_csv(\"submission_id.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(topics_dict[\"title\"]).to_csv(\"submisstion_titles.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_breadit",
   "language": "python",
   "name": "py3_breadit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
