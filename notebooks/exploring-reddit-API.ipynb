{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the reddit API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Go to root folder\n",
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using praw\n",
    "\n",
    "Praw is a good way to get easy access to Reddit data. [This](https://www.geeksforgeeks.org/python-praw-python-reddit-api-wrapper/) is a useful article. \n",
    "\n",
    "First, you will need to setup a connection to the Reddit API, hence you need to first [sign up and register your app with Reddit](https://www.reddit.com/wiki/api) to access their API.\n",
    "\n",
    "You then use your details to setup the connection. [This archived post](https://github.com/reddit-archive/reddit/wiki/OAuth2) also explains the authentification. I have stored the details in a configuration file which I load with ConfigParser. \n",
    "The file looks like this (example data) \n",
    "\n",
    "    [reddit-config]\n",
    "    client_id =  yourclientid\n",
    "    client_secret = yourclientsecret \n",
    "    user_agent = my user agent \n",
    "    username = yourusername \n",
    "    password = yourpassword\n",
    "\n",
    "\n",
    "Second, you need to install praw (check out the [official documentation](https://praw.readthedocs.io/en/latest/index.html) for help), and you are ready to go. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import configparser\n",
    "import praw\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve details from config file\n",
    "def get_config_values(config_file, section):\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(config_file)\n",
    "\n",
    "    return {\n",
    "        \"username\": config.get(section, 'username'),\n",
    "        \"password\": config.get(section, 'password'),\n",
    "        \"user_agent\": config.get(section, 'user_agent'),\n",
    "        \"client_id\": config.get(section, 'client_id'),\n",
    "        \"client_secret\": config.get(section, 'client_secret'),\n",
    "    }\n",
    "\n",
    "details = get_config_values(\"reddit-config.cfg\", \"reddit-config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# setup praw Reddit connection\n",
    "reddit = praw.Reddit(client_id = details[\"client_id\"], \n",
    "                     client_secret = details[\"client_secret\"], \n",
    "                     user_agent = details[\"user_agent\"], \n",
    "                     username = details[\"username\"], \n",
    "                     password = details[\"password\"]) \n",
    "  \n",
    "# to verify whether the instance is authorised instance or not \n",
    "print(reddit.read_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access a Subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sourdough\n",
      "Sourdough\n",
      "Want to learn how to make and bake sourdough? Love the aroma, taste, and texture of homemade bread? If yes, this is your subreddit!  Ask questions, start discussions, share recipes, photos, baking tips, and much more.\n",
      "\n",
      "***\n",
      "\n",
      "Get your own custom bread flair by clicking \"edit\" just above and selecting your flair.\n",
      "\n",
      "***\n",
      "\n",
      "**Rules:**\n",
      "\n",
      "* Be polite & respectful\n",
      "* No submitting irrelevant content (including memes)\n",
      "* No spamming\n",
      "* [Reddiquette](http://www.reddit.com/help/reddiquette)\n",
      "\n",
      "***\n",
      "\n",
      "**Resources:**\n",
      "\n",
      "* [Beginner's Guide to Sourdough](/r/Sourdough/wiki/starter_culture_resources)\n",
      "* [General Information on Sourdough](/r/Sourdough/wiki/general_information)\n",
      "* [Basic Troubleshooting](https://i.redd.it/w3ami1gnyqr41.jpg)\n",
      "* [Sourdough Recipe w/ Wakthrough](/r/Sourdough/wiki/standard-sd-recipe)\n",
      "\n",
      "***\n",
      "\n",
      "**Baking Related Subreddits:**\n",
      "\n",
      "* [/r/ArtisanBread](http://www.reddit.com/r/ArtisanBread/)\n",
      "* [/r/Breadit](http://www.reddit.com/r/Breadit/)\n",
      "* [/r/Pizza](http://www.reddit.com/r/Pizza/)\n",
      "* [/r/Baking](http://www.reddit.com/r/Baking/)\n",
      "\n",
      "***\n",
      "\n",
      "**Other Related Subreddits:**\n",
      "\n",
      "* [/r/AskCulinary ](http://www.reddit.com/r/AskCulinary/)\n",
      "* [/r/Charcuterie](http://www.reddit.com/r/charcuterie/)\n",
      "* [/r/Cheese](http://www.reddit.com/r/cheese/)\n",
      "* [/r/Condiments](http://www.reddit.com/r/condiments/)\n",
      "* [/r/EatSandwiches](http://www.reddit.com/r/eatsandwiches)\n",
      "* [/r/Fermentation](http://www.reddit.com/r/fermentation/)\n"
     ]
    }
   ],
   "source": [
    "subreddit = reddit.subreddit('sourdough') \n",
    "  \n",
    "# display the subreddit name \n",
    "print(subreddit.display_name) \n",
    "  \n",
    "# display the subreddit title  \n",
    "print(subreddit.title)        \n",
    "  \n",
    "# display the subreddit description  \n",
    "print(subreddit.description) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:  Here’s another video of me shaping sourdough. I added some music this time because baking is rock ’n roll.\n",
      "Score:  4435\n",
      "ID:  glzuwy\n",
      "URL:  https://v.redd.it/t8jaoor0giz41\n",
      "Created:  1589801997.0\n",
      "Number of comments:  214\n"
     ]
    }
   ],
   "source": [
    "# to find the top most submission in the subreddit \"sourdough\" \n",
    "subreddit = reddit.subreddit('sourdough') \n",
    "  \n",
    "for submission in subreddit.top(limit = 1): \n",
    "    # displays the submission title \n",
    "    print(\"Title: \", submission.title)   \n",
    "  \n",
    "    # displays the net upvotes of the submission \n",
    "    print(\"Score: \", submission.score)   \n",
    "  \n",
    "    # displays the submission's ID \n",
    "    print(\"ID: \", submission.id)    \n",
    "  \n",
    "    # displays the url of the submission \n",
    "    print(\"URL: \", submission.url) \n",
    "    \n",
    "    # displays when the submission was created in unix time\n",
    "    print(\"Created: \", submission.created_utc)  \n",
    "    \n",
    "    # displays number of comments to the submission\n",
    "    print(\"Number of comments: \", submission.num_comments) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['An olive rosemary loaf! First sourdough bake of the year & first time adding inclusions was a success. The aromatics & flavor of this loaf is insane, excited to experiment more w/ inclusions this year.',\n",
       " 'Delighted with the crumb on this one!',\n",
       " 'My first sourdough bread.',\n",
       " 'Sourdough newbie here',\n",
       " 'Below is a slice from the end, top is a slice from the middle of the same bread. What went wrong?']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to find the top most submission in the subreddit \"sourdough\" \n",
    "subreddit = reddit.subreddit('sourdough') \n",
    "\n",
    "df_title = []\n",
    "\n",
    "for submission in subreddit.new(limit = 5): \n",
    "    title = submission.title\n",
    "    df_title.append(title)\n",
    "\n",
    "df_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>created_UTC</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>comments_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Here’s another video of me shaping sourdough. ...</td>\n",
       "      <td>4435</td>\n",
       "      <td>1.589831e+09</td>\n",
       "      <td>214</td>\n",
       "      <td>This is hypnotizing, and I’m so impressed with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My dad decided to make a Coronavirus themed so...</td>\n",
       "      <td>3356</td>\n",
       "      <td>1.589723e+09</td>\n",
       "      <td>44</td>\n",
       "      <td>it's got such a 1990's aesthetic to it, i love...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>by request: a timelapse of me scoring my sunfl...</td>\n",
       "      <td>3294</td>\n",
       "      <td>1.584190e+09</td>\n",
       "      <td>104</td>\n",
       "      <td>Ngl I thought the cutting was over about 10 di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How else was I suppose to announce that we are...</td>\n",
       "      <td>2920</td>\n",
       "      <td>1.591844e+09</td>\n",
       "      <td>86</td>\n",
       "      <td>Congrats on the sex!\\nCongratulations on loaf ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Today is a good day.</td>\n",
       "      <td>2844</td>\n",
       "      <td>1.588969e+09</td>\n",
       "      <td>121</td>\n",
       "      <td>It's so pretty I want to cry\\nAmazing ! What w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I finally achieved what I thought was impossib...</td>\n",
       "      <td>2841</td>\n",
       "      <td>1.600390e+09</td>\n",
       "      <td>165</td>\n",
       "      <td>Do you use gluten free flour in the banneton? ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>That incredible moment when you take off the t...</td>\n",
       "      <td>2643</td>\n",
       "      <td>1.590562e+09</td>\n",
       "      <td>119</td>\n",
       "      <td>That bread has never even heard of gluten\\nOn ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>My pizza dough this morning</td>\n",
       "      <td>2536</td>\n",
       "      <td>1.593475e+09</td>\n",
       "      <td>78</td>\n",
       "      <td>holy gluten batman.\\nWould you mind sharing yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Hey guys here’s a video of me dividing and pre...</td>\n",
       "      <td>2520</td>\n",
       "      <td>1.590619e+09</td>\n",
       "      <td>109</td>\n",
       "      <td>I find every one of your videos so hypnoticall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>✨ a little project I did tonight for my kitche...</td>\n",
       "      <td>2337</td>\n",
       "      <td>1.593784e+09</td>\n",
       "      <td>113</td>\n",
       "      <td>[deleted]\\nYou forgot 10% Luck, 20% Skill\\nThi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  score   created_UTC  \\\n",
       "0  Here’s another video of me shaping sourdough. ...   4435  1.589831e+09   \n",
       "1  My dad decided to make a Coronavirus themed so...   3356  1.589723e+09   \n",
       "2  by request: a timelapse of me scoring my sunfl...   3294  1.584190e+09   \n",
       "3  How else was I suppose to announce that we are...   2920  1.591844e+09   \n",
       "4                               Today is a good day.   2844  1.588969e+09   \n",
       "5  I finally achieved what I thought was impossib...   2841  1.600390e+09   \n",
       "6  That incredible moment when you take off the t...   2643  1.590562e+09   \n",
       "7                        My pizza dough this morning   2536  1.593475e+09   \n",
       "8  Hey guys here’s a video of me dividing and pre...   2520  1.590619e+09   \n",
       "9  ✨ a little project I did tonight for my kitche...   2337  1.593784e+09   \n",
       "\n",
       "   num_comments                                      comments_text  \n",
       "0           214  This is hypnotizing, and I’m so impressed with...  \n",
       "1            44  it's got such a 1990's aesthetic to it, i love...  \n",
       "2           104  Ngl I thought the cutting was over about 10 di...  \n",
       "3            86  Congrats on the sex!\\nCongratulations on loaf ...  \n",
       "4           121  It's so pretty I want to cry\\nAmazing ! What w...  \n",
       "5           165  Do you use gluten free flour in the banneton? ...  \n",
       "6           119  That bread has never even heard of gluten\\nOn ...  \n",
       "7            78  holy gluten batman.\\nWould you mind sharing yo...  \n",
       "8           109  I find every one of your videos so hypnoticall...  \n",
       "9           113  [deleted]\\nYou forgot 10% Luck, 20% Skill\\nThi...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to find the top most submission in the subreddit \"sourdough\" \n",
    "subreddit = reddit.subreddit('sourdough') \n",
    "\n",
    "topics_dict = { \"title\":[], \n",
    "               \"score\":[],\n",
    "              \"created_UTC\": [],\n",
    "               \"num_comments\": [],\n",
    "               \"comments_text\": []}\n",
    "\n",
    "for submission in subreddit.top(limit = 10): \n",
    "    topics_dict[\"title\"].append(submission.title)\n",
    "    topics_dict[\"score\"].append(submission.score)\n",
    "    topics_dict[\"created_UTC\"].append(submission.created)\n",
    "    topics_dict[\"num_comments\"].append(submission.num_comments)\n",
    "    \n",
    "    #https://praw.readthedocs.io/en/latest/tutorials/comments.html\n",
    "    submission.comments.replace_more(limit=None)\n",
    "    comment_body = \"\"\n",
    "    for comment in submission.comments.list():\n",
    "        comment_body =  comment_body + comment.body + \"\\n\"\n",
    "    topics_dict[\"comments_text\"].append(comment_body)\n",
    "\n",
    "#convert dictionary to dataframe\n",
    "topics_data = pd.DataFrame(topics_dict)\n",
    "topics_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pushshift.io\n",
    "\n",
    "Using Praw is a very convenient way to access the Reddit API. However, it does not allow to filter for specific dates and has a data limit of 5000. More info about this change [here](https://stackoverflow.com/questions/53988619/praw-6-get-all-submission-of-a-subreddit).\n",
    "\n",
    "There is another way to access Reddit data via the [Pushshift API](https://pushshift.io/api-parameters/). This is what I tried to explore below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1\n",
    "\n",
    "Following [this](https://rareloot.medium.com/using-pushshifts-api-to-extract-reddit-submissions-fb517b286563) article but with small amends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "server status: 200\n",
      "start again at: 1578019458\n",
      "data loaded: 100\n",
      "server status: 200\n",
      "start again at: 1578095504\n",
      "data loaded: 139\n",
      "server status: 200\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import math\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "def getPushshiftData(start_at, end_at, subreddit):\n",
    "    url = 'https://api.pushshift.io/reddit/search/submission?&size=100&after='+str(start_at)+'&before='+str(end_at)+'&subreddit='+str(subreddit)\n",
    "    r = requests.get(url)\n",
    "    print('server status:', r.status_code)\n",
    "    \n",
    "    # if page available, run code as normal\n",
    "    if r.status_code == 200:\n",
    "        data = json.loads(r.text)\n",
    "        return data['data']\n",
    "    \n",
    "    # if page not able to load, wait 1 min and try again\n",
    "    else:\n",
    "        print(\"sleep 60\")\n",
    "        time.sleep(60)\n",
    "        url = 'https://api.pushshift.io/reddit/search/submission?&size=100&after='+str(start_at)+'&before='+str(end_at)+'&subreddit='+str(subreddit)\n",
    "        r = requests.get(url)\n",
    "        data = json.loads(r.text)\n",
    "        return data['data']\n",
    "        \n",
    "#dictionary to store values in\n",
    "post_dict = { \"id\" : [], \n",
    "             \"score\" :[],\n",
    "            \"created_utc\":[],\n",
    "             \"title\":[],\n",
    "             \"num_comments\" : [],\n",
    "             \"can_mod_post\": [],\n",
    "             \"author\":[]\n",
    "            }\n",
    "\n",
    "#define search parameters\n",
    "subreddit='Sourdough'\n",
    "start_at = str(math.ceil(datetime(2020, 1, 1, 0, 0, 0).timestamp()))\n",
    "end_at = str(math.floor(datetime(2020, 1, 3, 23, 59, 59).timestamp()))\n",
    "\n",
    "#retrieve data given the parameters\n",
    "data = getPushshiftData(start_at, end_at, subreddit)\n",
    "\n",
    "# Will run until all posts have been gathered from the 'start_at' date until the 'end_at' date\n",
    "while len(data) > 0:\n",
    "    for submission in data:\n",
    "        post_dict[\"id\"].append(submission[\"id\"])\n",
    "        post_dict[\"title\"].append(submission[\"title\"])\n",
    "        post_dict[\"created_utc\"].append(submission[\"created_utc\"])\n",
    "        post_dict[\"score\"].append(submission[\"score\"])\n",
    "        post_dict[\"num_comments\"].append(submission[\"num_comments\"])\n",
    "        post_dict[\"can_mod_post\"].append(submission[\"can_mod_post\"])\n",
    "        post_dict[\"author\"].append(submission[\"author\"])\n",
    "        \n",
    "    # Calls getPushshiftData() with the created date of the last submission\n",
    "    print('start again at:', data[-1]['created_utc'])\n",
    "    print('data loaded:', len(post_dict[\"title\"]))\n",
    "    time.sleep(15)\n",
    "    data = getPushshiftData(subreddit=subreddit, start_at=data[-1]['created_utc'], end_at=end_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>title</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>can_mod_post</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eibhvl</td>\n",
       "      <td>1</td>\n",
       "      <td>1577839131</td>\n",
       "      <td>First attempt at a starter, really hope I mana...</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>coentertainer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eibvur</td>\n",
       "      <td>1</td>\n",
       "      <td>1577841129</td>\n",
       "      <td>Skillet &amp;amp; Dutch Oven Sourdough in the rain...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Richness69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eiby7m</td>\n",
       "      <td>1</td>\n",
       "      <td>1577841483</td>\n",
       "      <td>My last bread of 2019. I used Brad and Claire’...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>canioli019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eictkk</td>\n",
       "      <td>1</td>\n",
       "      <td>1577846281</td>\n",
       "      <td>I started baking in September and I have never...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>singular-chip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eidmqm</td>\n",
       "      <td>1</td>\n",
       "      <td>1577851082</td>\n",
       "      <td>Sourdough Books</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>TheNightBaker97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>eidtic</td>\n",
       "      <td>1</td>\n",
       "      <td>1577852213</td>\n",
       "      <td>Analyzing sourdough?</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>amisanyal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>eidxpd</td>\n",
       "      <td>1</td>\n",
       "      <td>1577852956</td>\n",
       "      <td>Ginger tumeric loaf to guide me out of the decade</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>bleuxballs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>eidyxu</td>\n",
       "      <td>1</td>\n",
       "      <td>1577853173</td>\n",
       "      <td>Behold Bread Majors. He will incite the Rocky ...</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>ClandestineOni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>eifrvq</td>\n",
       "      <td>1</td>\n",
       "      <td>1577864698</td>\n",
       "      <td>Last loaves of the year.</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>gorpz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>eigw2g</td>\n",
       "      <td>1</td>\n",
       "      <td>1577873535</td>\n",
       "      <td>Wheat flour starter vs rye starter</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "      <td>bacafreak</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  score  created_utc  \\\n",
       "0  eibhvl      1   1577839131   \n",
       "1  eibvur      1   1577841129   \n",
       "2  eiby7m      1   1577841483   \n",
       "3  eictkk      1   1577846281   \n",
       "4  eidmqm      1   1577851082   \n",
       "5  eidtic      1   1577852213   \n",
       "6  eidxpd      1   1577852956   \n",
       "7  eidyxu      1   1577853173   \n",
       "8  eifrvq      1   1577864698   \n",
       "9  eigw2g      1   1577873535   \n",
       "\n",
       "                                               title  num_comments  \\\n",
       "0  First attempt at a starter, really hope I mana...             5   \n",
       "1  Skillet &amp; Dutch Oven Sourdough in the rain...             0   \n",
       "2  My last bread of 2019. I used Brad and Claire’...             0   \n",
       "3  I started baking in September and I have never...             0   \n",
       "4                                    Sourdough Books             3   \n",
       "5                               Analyzing sourdough?             1   \n",
       "6  Ginger tumeric loaf to guide me out of the decade             2   \n",
       "7  Behold Bread Majors. He will incite the Rocky ...             3   \n",
       "8                           Last loaves of the year.             0   \n",
       "9                 Wheat flour starter vs rye starter             9   \n",
       "\n",
       "   can_mod_post           author  \n",
       "0         False    coentertainer  \n",
       "1         False       Richness69  \n",
       "2         False       canioli019  \n",
       "3         False    singular-chip  \n",
       "4         False  TheNightBaker97  \n",
       "5         False        amisanyal  \n",
       "6         False       bleuxballs  \n",
       "7         False   ClandestineOni  \n",
       "8         False            gorpz  \n",
       "9         False        bacafreak  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert dictionary to dataframe\n",
    "post_df = pd.DataFrame(post_dict)\n",
    "\n",
    "post_df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Article 2 \n",
    "https://medium.com/@pasdan/how-to-scrap-reddit-using-pushshift-io-via-python-a3ebcc9b83f4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import requests\n",
    "import itertools\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create logic to retrieve more data above limit\n",
    "\n",
    "def make_request(uri, max_retries = 5):\n",
    "    def fire_away(uri):\n",
    "        response = requests.get(uri)\n",
    "        assert response.status_code == 200\n",
    "        return json.loads(response.content)    \n",
    "    current_tries = 1\n",
    "    while current_tries < max_retries:\n",
    "        try:\n",
    "            time.sleep(1)\n",
    "            response = fire_away(uri)\n",
    "            return response\n",
    "        except:\n",
    "            time.sleep(1)\n",
    "            current_tries += 1    \n",
    "    return fire_away(uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_posts_for(subreddit, start_at, end_at):\n",
    "    \n",
    "    def map_posts(posts):\n",
    "        return list(map(lambda post: {\n",
    "            'id': post['id'],\n",
    "            'created_utc': post['created_utc'],\n",
    "            'prefix': 't4_'\n",
    "        }, posts))\n",
    "    \n",
    "    SIZE = 500\n",
    "    URI_TEMPLATE = r'https://api.pushshift.io/reddit/search/submission?subreddit={}&after={}&before={}&size={}'\n",
    "    \n",
    "    post_collections = map_posts( \\\n",
    "        make_request(URI_TEMPLATE.format(subreddit, start_at, end_at, SIZE))['data'])    \n",
    "    \n",
    "    n = len(post_collections)\n",
    "    while n == SIZE:\n",
    "        last = post_collections[-1]\n",
    "        new_start_at = last['created_utc'] - (10)\n",
    "        \n",
    "        more_posts = map_posts( \\\n",
    "            make_request(URI_TEMPLATE.format(subreddit, new_start_at, end_at, SIZE))['data'])\n",
    "        \n",
    "        n = len(more_posts)\n",
    "        post_collections.extend(more_posts)\n",
    "        \n",
    "    return post_collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building time search intervals\n",
    "def give_me_intervals(start_at, end_at, days_per_interval): \n",
    "           \n",
    "    period = (86400 * days_per_interval)    ## 1 day = 86400\n",
    "    end = start_at + period\n",
    "    yield (int(start_at), int(end))    \n",
    "    padding = 1 \n",
    "    while end <= end_at:\n",
    "        start_at = end + padding\n",
    "        end = (start_at - padding) + period\n",
    "        yield int(start_at), int(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length: 732\n",
      "[(1577836800, 1578441600), (1578441601, 1579046400), (1579046401, 1579651200), (1579651201, 1580256000), (1580256001, 1580860800), (1580860801, 1581465600), (1581465601, 1582070400), (1582070401, 1582675200), (1582675201, 1583280000), (1583280001, 1583884800), (1583884801, 1584489600), (1584489601, 1585094400), (1585094401, 1585699200), (1585699201, 1586304000), (1586304001, 1586908800), (1586908801, 1587513600), (1587513601, 1588118400), (1588118401, 1588723200), (1588723201, 1589328000), (1589328001, 1589932800), (1589932801, 1590537600), (1590537601, 1591142400), (1591142401, 1591747200), (1591747201, 1592352000), (1592352001, 1592956800), (1592956801, 1593561600), (1593561601, 1594166400), (1594166401, 1594771200), (1594771201, 1595376000), (1595376001, 1595980800), (1595980801, 1596585600), (1596585601, 1597190400), (1597190401, 1597795200), (1597795201, 1598400000), (1598400001, 1599004800), (1599004801, 1599609600), (1599609601, 1600214400), (1600214401, 1600819200), (1600819201, 1601424000), (1601424001, 1602028800), (1602028801, 1602633600), (1602633601, 1603238400), (1603238401, 1603843200), (1603843201, 1604448000), (1604448001, 1605052800), (1605052801, 1605657600), (1605657601, 1606262400), (1606262401, 1606867200), (1606867201, 1607472000), (1607472001, 1608076800), (1608076801, 1608681600), (1608681601, 1609286400), (1609286401, 1609891200)]\n"
     ]
    }
   ],
   "source": [
    "## test function\n",
    "start_at = math.floor((datetime(2020, 1, 1, 0, 0, 0)).timestamp())\n",
    "end_at = math.ceil(c\n",
    "\n",
    "print(\"length:\", len(list(give_me_intervals(start_at, end_at, 0.5))))\n",
    "print(list(give_me_intervals(start_at,end_at, 7)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3146\n",
      "3146\n"
     ]
    }
   ],
   "source": [
    "## Pull submissions\n",
    "\n",
    "# Define parameters\n",
    "#To be safe, I changed the day interval to 1/2 day, in case any day had more than 100 posts\n",
    "subreddit = 'Sourdough'\n",
    "start_at = math.floor((datetime(2020, 1, 1, 0, 0, 0)).timestamp())\n",
    "end_at = math.ceil(datetime(2020, 12, 31, 23, 59, 59).timestamp()) \n",
    "days_per_interval = 0.5\n",
    "\n",
    "posts = []\n",
    "for interval in give_me_intervals(start_at, end_at, days_per_interval):\n",
    "    pulled_posts = pull_posts_for(subreddit, interval[0], interval[1])\n",
    "    posts.extend(pulled_posts)\n",
    "    time.sleep(.500)\n",
    "\n",
    "# check results\n",
    "print(len(posts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# write the result to a file in case I want to use it later to avoid having to rerun the code above\n",
    "import pickle\n",
    "\n",
    "my_object = posts\n",
    "pickle_out = open(\"posts_list.pickle\",\"wb\")\n",
    "pickle.dump(my_object, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'eibhvl', 'created_utc': 1577839131, 'prefix': 't4_'},\n",
       " {'id': 'eibvur', 'created_utc': 1577841129, 'prefix': 't4_'},\n",
       " {'id': 'eiby7m', 'created_utc': 1577841483, 'prefix': 't4_'},\n",
       " {'id': 'eictkk', 'created_utc': 1577846281, 'prefix': 't4_'},\n",
       " {'id': 'eidmqm', 'created_utc': 1577851082, 'prefix': 't4_'},\n",
       " {'id': 'eidtic', 'created_utc': 1577852213, 'prefix': 't4_'},\n",
       " {'id': 'eidxpd', 'created_utc': 1577852956, 'prefix': 't4_'},\n",
       " {'id': 'eidyxu', 'created_utc': 1577853173, 'prefix': 't4_'},\n",
       " {'id': 'eifrvq', 'created_utc': 1577864698, 'prefix': 't4_'},\n",
       " {'id': 'eigw2g', 'created_utc': 1577873535, 'prefix': 't4_'}]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Unpickling: read result back in using pickle\n",
    "#pickle_in = open(\"posts_list.pickle\",\"rb\")\n",
    "#test_object = pickle.load(pickle_in)\n",
    "#test_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3146\n"
     ]
    }
   ],
   "source": [
    "## get more data from reddit using praw\n",
    "# Generate list including Submissions and their id to then get the rest of the data from\n",
    "posts_from_reddit = []\n",
    "\n",
    "for submission_id in np.unique([ post['id'] for post in posts ]):\n",
    "    submission = reddit.submission(id=submission_id) \n",
    "    \n",
    "    posts_from_reddit.append(submission)  \n",
    "\n",
    "print(len(posts_from_reddit))\n",
    "print(posts_from_reddit[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull other data from reddit including title, score, date, number of comments, etc\n",
    "posts_dict = { \"title\":[], \n",
    "               \"score\":[],\n",
    "              \"created\": [],\n",
    "               \"created_UTC\": [],\n",
    "               \"num_comments\": [],\n",
    "               \"comments_text\": []\n",
    "              }\n",
    "\n",
    "for submission in posts_from_reddit:\n",
    "    topics_dict[\"title\"].append(submission.title)\n",
    "    topics_dict[\"score\"].append(submission.score)\n",
    "    topics_dict[\"created_UTC\"].append(submission.created)\n",
    "    topics_dict[\"num_comments\"].append(submission.num_comments)\n",
    "\n",
    "    \n",
    "#convert dictionary to dataframe\n",
    "posts_data = pd.DataFrame(posts_dict)\n",
    "posts_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert unix time into date --> not working properly, need to fix unix conversion\n",
    "def get_yyyy_mm_dd_from_utc(dt):\n",
    "    date = datetime.utcfromtimestamp(dt)\n",
    "    return str(date.year) + \"-\" + str(date.month) + \"-\" + str(date.day)\n",
    "\n",
    "created = []\n",
    "\n",
    "for submission in posts_from_reddit:\n",
    "    created.append(get_yyyy_mm_dd_from_utc(submission.created))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(created).to_csv(\"date.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list(np.unique([ post['id'] for post in posts ]))).to_csv(\"submission_id.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(topics_dict[\"title\"]).to_csv(\"submisstion_titles.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_breadit",
   "language": "python",
   "name": "py3_breadit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
